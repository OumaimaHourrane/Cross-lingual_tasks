{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "co7MV6sX7Xto"
   },
   "source": [
    "# Cross-Lingual Similarity and Semantic Search Engine with Multilingual Universal Sentence Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "id": "lVjNK8shFKOC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_text in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (2.4.3)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from tensorflow_text) (0.11.0)\n",
      "Requirement already satisfied: tensorflow<2.5,>=2.4.0 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from tensorflow_text) (2.4.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from tensorflow-hub>=0.8.0->tensorflow_text) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from tensorflow-hub>=0.8.0->tensorflow_text) (3.15.6)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (2.4.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.1.2)\n",
      "Requirement already satisfied: six~=1.15.0 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.15.0)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.32.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.1.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (0.36.2)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (0.3.3)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (3.7.4.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.12)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (2.10.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.6.3)\n",
      "Requirement already satisfied: absl-py~=0.10 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (0.12.0)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (2.4.1)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (3.3.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (0.2.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (0.4.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (2.25.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /snap/jupyter/6/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (41.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.27.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.7.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.26.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /snap/jupyter/6/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (2019.3.9)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (4.2.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.4.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (0.4.8)\n",
      "Requirement already satisfied: bokeh in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (2.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /snap/jupyter/6/lib/python3.7/site-packages (from bokeh) (2.8.0)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from bokeh) (8.1.2)\n",
      "Requirement already satisfied: packaging>=16.8 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from bokeh) (20.9)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from bokeh) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from bokeh) (1.19.5)\n",
      "Requirement already satisfied: tornado>=5.1 in /snap/jupyter/6/lib/python3.7/site-packages (from bokeh) (6.0.2)\n",
      "Requirement already satisfied: Jinja2>=2.7 in /snap/jupyter/6/lib/python3.7/site-packages (from bokeh) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from bokeh) (3.7.4.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from python-dateutil>=2.1->bokeh) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (from packaging>=16.8->bokeh) (2.4.7)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /snap/jupyter/6/lib/python3.7/site-packages (from Jinja2>=2.7->bokeh) (1.1.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: simpleneighbors[annoy] in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (0.1.0)\n",
      "Collecting annoy>=1.16.0; extra == \"annoy\" (from simpleneighbors[annoy])\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/5b/1c22129f608b3f438713b91cd880dc681d747a860afe3e8e0af86e921942/annoy-1.17.0.tar.gz (646kB)\n",
      "\u001b[K     |████████████████████████████████| 655kB 639kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: annoy\n",
      "  Building wheel for annoy (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Complete output from command /snap/jupyter/6/bin/python -u -c 'import setuptools, tokenize;__file__='\"'\"'/tmp/pip-install-sog47k_n/annoy/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-e431oyqc --python-tag cp37:\u001b[0m\n",
      "\u001b[31m  ERROR: running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.linux-x86_64-3.7\n",
      "  creating build/lib.linux-x86_64-3.7/annoy\n",
      "  copying annoy/__init__.py -> build/lib.linux-x86_64-3.7/annoy\n",
      "  running build_ext\n",
      "  building 'annoy.annoylib' extension\n",
      "  creating build/temp.linux-x86_64-3.7\n",
      "  creating build/temp.linux-x86_64-3.7/src\n",
      "  gcc -pthread -B /home/filipe/miniconda3/envs/JUPYTER/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/snap/jupyter/6/include/python3.7m -c src/annoymodule.cc -o build/temp.linux-x86_64-3.7/src/annoymodule.o -D_CRT_SECURE_NO_WARNINGS -march=native -O3 -ffast-math -fno-associative-math -DANNOYLIB_MULTITHREADED_BUILD -std=c++14\n",
      "  unable to execute 'gcc': No such file or directory\n",
      "  error: command 'gcc' failed with exit status 1\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for annoy\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for annoy\n",
      "Failed to build annoy\n",
      "Installing collected packages: annoy\n",
      "  Running setup.py install for annoy ... \u001b[?25lerror\n",
      "\u001b[31m    ERROR: Complete output from command /snap/jupyter/6/bin/python -u -c 'import setuptools, tokenize;__file__='\"'\"'/tmp/pip-install-sog47k_n/annoy/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-rp2m7h02/install-record.txt --single-version-externally-managed --compile --user --prefix=:\u001b[0m\n",
      "\u001b[31m    ERROR: running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build/lib.linux-x86_64-3.7\n",
      "    creating build/lib.linux-x86_64-3.7/annoy\n",
      "    copying annoy/__init__.py -> build/lib.linux-x86_64-3.7/annoy\n",
      "    running build_ext\n",
      "    building 'annoy.annoylib' extension\n",
      "    creating build/temp.linux-x86_64-3.7\n",
      "    creating build/temp.linux-x86_64-3.7/src\n",
      "    gcc -pthread -B /home/filipe/miniconda3/envs/JUPYTER/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/snap/jupyter/6/include/python3.7m -c src/annoymodule.cc -o build/temp.linux-x86_64-3.7/src/annoymodule.o -D_CRT_SECURE_NO_WARNINGS -march=native -O3 -ffast-math -fno-associative-math -DANNOYLIB_MULTITHREADED_BUILD -std=c++14\n",
      "    unable to execute 'gcc': No such file or directory\n",
      "    error: command 'gcc' failed with exit status 1\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command \"/snap/jupyter/6/bin/python -u -c 'import setuptools, tokenize;__file__='\"'\"'/tmp/pip-install-sog47k_n/annoy/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-rp2m7h02/install-record.txt --single-version-externally-managed --compile --user --prefix=\" failed with error code 1 in /tmp/pip-install-sog47k_n/annoy/\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/oumaima/snap/jupyter/common/lib/python3.7/site-packages (4.59.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_text\n",
    "!pip install simpleneighbors[annoy]\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oumaima/snap/jupyter/common/lib/python3.7/site-packages/joblib/_multiprocessing_helpers.py:45: UserWarning: [Errno 13] Permission denied.  joblib will operate in serial mode\n",
      "  warnings.warn('%s.  joblib will operate in serial mode' % (e,))\n"
     ]
    }
   ],
   "source": [
    "# Loading the Pre-trained model\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow_text import SentencepieceTokenizer\n",
    "import sklearn.metrics.pairwise\n",
    "\n",
    "from simpleneighbors import SimpleNeighbors\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "module_url = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3' \n",
    "\n",
    "model = hub.load(module_url)\n",
    "\n",
    "def embed_text(input):\n",
    "    return model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxAFAJI9xsAU"
   },
   "source": [
    "# Creating a Multilingual Semantic-Similarity Search Engine\n",
    "\n",
    "\n",
    "## Download Data to Index - news sentences in multiples languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "587I9ye6yXEU",
    "outputId": "a916a0d8-6cf3-4153-ac5c-f5613ac54f94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://opus.nlpl.eu/download.php?f=News-Commentary/v11/moses/ar-en.txt.zip\n",
      "24715264/24714354 [==============================] - 24s 1us/step\n",
      "1,000 Arabic sentences\n",
      "Downloading data from http://opus.nlpl.eu/download.php?f=News-Commentary/v11/moses/en-zh.txt.zip\n",
      "18104320/18101984 [==============================] - 17s 1us/step\n",
      "1,000 Chinese sentences\n",
      "Downloading data from http://opus.nlpl.eu/download.php?f=News-Commentary/v11/moses/en-es.txt.zip\n",
      "28106752/28106064 [==============================] - 27s 1us/step\n",
      "1,000 English sentences\n",
      "Downloading data from http://opus.nlpl.eu/download.php?f=News-Commentary/v11/moses/en-ru.txt.zip\n",
      "24854528/24849511 [==============================] - 24s 1us/step\n",
      "1,000 Russian sentences\n",
      "1,000 Spanish sentences\n"
     ]
    }
   ],
   "source": [
    "corpus_metadata = [\n",
    "    ('ar', 'ar-en.txt.zip', 'News-Commentary.ar-en.ar', 'Arabic'),\n",
    "    ('zh', 'en-zh.txt.zip', 'News-Commentary.en-zh.zh', 'Chinese'),\n",
    "    ('en', 'en-es.txt.zip', 'News-Commentary.en-es.en', 'English'),\n",
    "    ('ru', 'en-ru.txt.zip', 'News-Commentary.en-ru.ru', 'Russian'),\n",
    "    ('es', 'en-es.txt.zip', 'News-Commentary.en-es.es', 'Spanish'),\n",
    "]\n",
    "\n",
    "language_to_sentences = {}\n",
    "language_to_news_path = {}\n",
    "for language_code, zip_file, news_file, language_name in corpus_metadata:\n",
    "    zip_path = tf.keras.utils.get_file(\n",
    "      fname=zip_file,\n",
    "      origin='http://opus.nlpl.eu/download.php?f=News-Commentary/v11/moses/' + zip_file,\n",
    "      extract=True)\n",
    "    news_path = os.path.join(os.path.dirname(zip_path), news_file)\n",
    "    language_to_sentences[language_code] = pd.read_csv(news_path, sep='\\t', header=None)[0][:1000]\n",
    "    language_to_news_path[language_code] = news_path\n",
    "\n",
    "    print('{:,} {} sentences'.format(len(language_to_sentences[language_code]), language_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3DIT9uT7Z34"
   },
   "source": [
    "## Using the pre-trained model to encode sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "yRoRT5qCEIYy",
    "outputId": "09d7cfd9-284f-421b-fef9-bbd1806b1bb3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing Arabic embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10240it [00:50, 197.02it/s]             "
     ]
    }
   ],
   "source": [
    "batch_size = 2048\n",
    "language_to_embeddings = {}\n",
    "for language_code, zip_file, news_file, language_name in corpus_metadata:\n",
    "    print('\\nComputing {} embeddings'.format(language_name))\n",
    "    with tqdm(total=len(language_to_sentences[language_code])) as pbar:\n",
    "        for batch in pd.read_csv(language_to_news_path[language_code], sep='\\t',header=None, chunksize=batch_size):\n",
    "            language_to_embeddings.setdefault(language_code, []).extend(embed_text(batch[0]))\n",
    "            pbar.update(len(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oeBqoE8e-scg"
   },
   "source": [
    "## Building an index of semantic vectors using SimpleNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "lv_SOduAF1oi",
    "outputId": "17d283d3-3320-4166-802b-cfff2e41fce4"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "num_index_trees = 40\n",
    "language_name_to_index = {}\n",
    "embedding_dimensions = len(list(language_to_embeddings.values())[0][0])\n",
    "for language_code, zip_file, news_file, language_name in corpus_metadata:\n",
    "    print('\\nAdding {} embeddings to index'.format(language_name))\n",
    "    index = SimpleNeighbors(embedding_dimensions, metric='dot')\n",
    "\n",
    "    for i in trange(len(language_to_sentences[language_code])):\n",
    "        index.add_one(language_to_sentences[language_code][i], language_to_embeddings[language_code][i])\n",
    "\n",
    "    print('Building {} index with {} trees...'.format(language_name, num_index_trees))\n",
    "    index.build(n=num_index_trees)\n",
    "    language_name_to_index[language_name] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "0aqGwIuLGrtu",
    "outputId": "63d7cff5-7442-423a-b75f-eca3aaf3fc22"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "num_index_trees = 60\n",
    "print('Computing mixed-language index')\n",
    "combined_index = SimpleNeighbors(embedding_dimensions, metric='dot')\n",
    "for language_code, zip_file, news_file, language_name in corpus_metadata:\n",
    "    print('Adding {} embeddings to mixed-language index'.format(language_name))\n",
    "    for i in trange(len(language_to_sentences[language_code])):\n",
    "        annotated_sentence = '({}) {}'.format(language_name, language_to_sentences[language_code][i])\n",
    "        combined_index.add_one(annotated_sentence, language_to_embeddings[language_code][i])\n",
    "\n",
    "print('Building mixed-language index with {} trees...'.format(num_index_trees))\n",
    "combined_index.build(n=num_index_trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dxu66S8wJIG9"
   },
   "source": [
    "### Testing Semantic-search cross-lingual capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "_EFSd65B_mq8",
    "outputId": "c14e25e6-ab77-4ee3-acc6-32e89a9e31d0"
   },
   "outputs": [],
   "source": [
    "sample_query = 'Global warming' \n",
    "index_language = 'English'  #[\"Arabic\", \"Chinese\", \"English\", \"French\", \"German\", \"Russian\", \"Spanish\"]\n",
    "num_results = 10  \n",
    "\n",
    "query_embedding = embed_text(sample_query)[0]\n",
    "search_results = language_name_to_index[index_language].nearest(query_embedding, n=num_results)\n",
    "\n",
    "print('{} sentences similar to: \"{}\"\\n'.format(index_language, sample_query))\n",
    "search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ybgj9o7hKDZV"
   },
   "source": [
    "### Mixed-corpus capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 751
    },
    "id": "MJeTzuj0KU41",
    "outputId": "ac0ccd52-5778-4019-e1da-8f73d5b278f9"
   },
   "outputs": [],
   "source": [
    "sample_query = 'Global warming'  \n",
    "num_results = 40  \n",
    "\n",
    "query_embedding = embed_text(sample_query)[0]\n",
    "search_results = language_name_to_index[index_language].nearest(query_embedding, n=num_results)\n",
    "\n",
    "print('{} sentences similar to: \"{}\"\\n'.format(index_language, sample_query))\n",
    "search_results"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Cross-Lingual Similarity and Semantic Search Engine with TF-Hub Multilingual Universal Encoder",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
